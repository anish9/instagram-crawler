{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. url, likes crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from inscrawler import secret\n",
    "\n",
    "\n",
    "username = 'streetstylestars'\n",
    "\n",
    "\n",
    "def find_one(driver, css_selector, waittime=0, elem=None):\n",
    "    if waittime:\n",
    "        WebDriverWait(driver, waittime).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, css_selector))\n",
    "        )\n",
    "    return driver.find_element(By.CSS_SELECTOR, css_selector)\n",
    "\n",
    "\n",
    "# Settings\n",
    "service_args = [\"--ignore-ssl-errors=true\"]\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Open browser\n",
    "driver = webdriver.Chrome(\n",
    "    executable_path=\"inscrawler/bin/chromedriver\",\n",
    "    service_args=service_args,\n",
    "    chrome_options=chrome_options,\n",
    ")\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# log in instagram\n",
    "url = 'https://www.instagram.com/accounts/login'\n",
    "driver.get(url)\n",
    "\n",
    "u_input = find_one(driver, 'input[name=\"username\"]', 0.5)\n",
    "u_input.send_keys(secret.username)\n",
    "p_input = find_one(driver, 'input[name=\"password\"]', 0.5)\n",
    "p_input.send_keys(secret.password)\n",
    "\n",
    "login_btn = find_one(driver, \".L3NKy\", 0.5)\n",
    "login_btn.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Get target page\n",
    "url = 'https://www.instagram.com/' + username\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Render enough posts\n",
    "for i in range(15):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart here when TimeoutException\n",
    "## (before run, manually click 'X' - exit button at the browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 50th src is crawled successfully\n",
      ">>> 100th src is crawled successfully\n",
      ">>> 150th src is crawled successfully\n",
      "\n",
      "===== crawling successfully finished =====\n"
     ]
    }
   ],
   "source": [
    "# click first post\n",
    "driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "ele_post = find_one(driver, \".v1Nh3 a\", 0.5)\n",
    "ele_post.click()\n",
    "time.sleep(1.5)\n",
    "\n",
    "# crawl url, likes of 180 posts\n",
    "res_list = []\n",
    "for i in range(180):\n",
    "    tmp_dict = {}\n",
    "    \n",
    "    tmp_url = None\n",
    "    likes = 0\n",
    "    try:\n",
    "        ele_img = find_one(driver, '._97aPb img', 10)\n",
    "        tmp_url = ele_img.get_attribute(\"src\")\n",
    "        \n",
    "        ele_likes = find_one(driver, '.Nm9Fw > * > span', 10)\n",
    "        likes = ele_likes.text\n",
    "        likes = int(likes.replace(\",\", \"\").replace(\".\", \"\")) if likes is not None else 0\n",
    "    except:\n",
    "        pass\n",
    "    tmp_dict['img_url'] = tmp_url\n",
    "    tmp_dict['likes'] = likes\n",
    "    \n",
    "    res_list.append(tmp_dict)\n",
    "    \n",
    "    # move to next post\n",
    "    next_btn = find_one(driver, \".EfHg9 ._65Bje.coreSpriteRightPaginationArrow\", 0.5)\n",
    "    next_btn.click()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # process check\n",
    "    if (i+1)%50==0:\n",
    "        print(f'>>> {i+1}th src is crawled successfully')\n",
    "\n",
    "print('\\n===== crawling successfully finished =====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> # of items    : 180\n"
     ]
    }
   ],
   "source": [
    "print(f'>>> # of items    : {len(res_list)}')\n",
    "\n",
    "out = json.dumps(res_list, ensure_ascii=False)\n",
    "with open('./output/output_'+username, \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =====================================\n",
    "## 2. picture crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json file successfully loaded\n",
      "Directory  ./output/pictures_fashionandstyle.official  Created\n",
      ">>> picture 50 has been successfully downloaded.\n",
      ">>> picture 100 has been successfully downloaded.\n",
      ">>> picture 150 has been successfully downloaded.\n",
      "download successfully completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def retrieve_pictures(username, likes_threshold=0, no_pictures=150):\n",
    "    # Open the output binary file of crawler.py\n",
    "    with open('./output/output_'+username, encoding='utf-8') as fp:\n",
    "        data = fp.read()\n",
    "        pass\n",
    "    data_list = json.loads(data)\n",
    "    print(\"json file successfully loaded\")\n",
    "\n",
    "    # Make a new directory to save pictures\n",
    "    dir_path = './output/pictures_'+username\n",
    "    if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "            print(\"Directory \" , dir_path ,  \" Created\")\n",
    "    else:    \n",
    "            print(\"Directory \" , dir_path ,  \" already exists\")\n",
    "\n",
    "    # retrieve & save pictures from instagram\n",
    "    # (only picutres with likes more than likes numbers)\n",
    "    success_num = 0\n",
    "    for idx, item in enumerate(data_list):\n",
    "        idx += 1\n",
    "\n",
    "        tmp_likes = item['likes']\n",
    "        if tmp_likes < likes_threshold:\n",
    "            continue\n",
    "\n",
    "        if item['img_url'] is not None:\n",
    "            tmp_url = item['img_url']\n",
    "\n",
    "            title_idx = '00'+str(idx)\n",
    "            title_idx = title_idx[-3:]\n",
    "            urlretrieve(tmp_url, dir_path+f'/{username}_{title_idx}_{tmp_likes}Likes.png')\n",
    "            \n",
    "            success_num += 1\n",
    "\n",
    "        if (idx+1) % 50 == 0:\n",
    "            print(f'>>> picture {idx+1} has been successfully downloaded.')\n",
    "        \n",
    "        if success_num == no_pictures:\n",
    "            break\n",
    "        pass\n",
    "    print(f'download successfully completed')\n",
    "\n",
    "\n",
    "retrieve_pictures(username)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
